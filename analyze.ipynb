{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "from datetime import datetime\n",
    "import mne\n",
    "from mne.preprocessing import ICA\n",
    "from scipy import signal\n",
    "import warnings\n",
    "import traceback\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "log_path = \"data_logs/Aws Safi\"\n",
    "data_path = \"data\"\n",
    "\n",
    "results_dir = \"results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 3 log files\n"
     ]
    }
   ],
   "source": [
    "def mp(c):\n",
    "    \"\"\"Convert log color markers to valid Plotly colors\"\"\"\n",
    "    if c.strip()==\"'blue'\":\n",
    "        return 'blue'\n",
    "    if c.strip()==\"'lightblue'\":\n",
    "        return 'red'\n",
    "    return 'black'\n",
    "\n",
    "logs = {}\n",
    "for log_file in os.listdir(log_path):\n",
    "    if not log_file.endswith('.txt'):\n",
    "        continue\n",
    "    identifier = log_file.split(\"_run\")[1].split(\".\")[0]\n",
    "    try:\n",
    "        with open(f\"{log_path}/{log_file}\", \"r\") as f:\n",
    "            a = f.readlines()\n",
    "            if not a:\n",
    "                print(f\"WARNING: Empty log file: {log_file}\")\n",
    "                continue\n",
    "            logs[identifier] = [\n",
    "                (datetime.strptime(line.split(\" \")[0], \"[%Y-%m-%dT%H:%M:%S.%fZ]\").timestamp(),\n",
    "                 mp(line.split(\" \")[-1]),\n",
    "                 \" \".join(line.split(\" \")[2:]))\n",
    "                for line in a\n",
    "                if 'blue' in line or '+' in line\n",
    "            ]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR parsing log file {log_file}: {str(e)}\")\n",
    "\n",
    "print(f\"Successfully parsed {len(logs)} log files\")\n",
    "\n",
    "def validate_csv(file_path):\n",
    "    \"\"\"Validate if a CSV file is readable and has data\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            return False, \"File does not exist\"\n",
    "\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            return False, \"File is empty\"\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            first_lines = []\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 5:\n",
    "                    break\n",
    "                first_lines.append(line)\n",
    "\n",
    "            if not first_lines:\n",
    "                return False, \"File has no content\"\n",
    "\n",
    "            if not ',' in first_lines[0]:\n",
    "                return False, \"File does not appear to be a valid CSV\"\n",
    "            l = f.readlines()\n",
    "            if not l[0].startswith('Timestamp'):\n",
    "                l = l[1:]\n",
    "            with open(f\"{file_path}\", 'w') as f:\n",
    "                f.writelines(l)\n",
    "\n",
    "        try:\n",
    "            df_sample = pd.read_csv(file_path, nrows=5)\n",
    "            if df_sample.empty:\n",
    "                return False, \"Pandas could not read any rows\"\n",
    "            return True, \"Valid\"\n",
    "        except pd.errors.EmptyDataError:\n",
    "            return False, \"No columns to parse from file\"\n",
    "        except pd.errors.ParserError:\n",
    "            return False, \"Parser error - file may be corrupted\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Error reading with pandas: {str(e)}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"Validation error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eeg(file_path, identifier, logs):\n",
    "    \"\"\"Process EEG data and print findings\"\"\"\n",
    "    print(f\"\\n{'='*80}\\nProcessing file: {os.path.basename(file_path)}\\n{'='*80}\")\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Successfully read data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not read data: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        if df.empty:\n",
    "            print(\"ERROR: DataFrame is empty\")\n",
    "            return None\n",
    "\n",
    "        if 'Timestamp' in df.columns:\n",
    "            df.set_index('Timestamp', inplace=True)\n",
    "            print(\"Set 'Timestamp' as index\")\n",
    "\n",
    "        eeg_channels = [col for col in df.columns if col.startswith('EEG.') and\n",
    "                       not any(x in col for x in ['Counter', 'Interpolated', 'RawCq', 'Battery', 'MarkerHardware'])]\n",
    "\n",
    "        if not eeg_channels:\n",
    "            print(\"ERROR: No EEG channels found in data\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Found {len(eeg_channels)} EEG channels: {', '.join(eeg_channels)}\")\n",
    "\n",
    "        cq_columns = [col for col in df.columns if col.startswith('CQ.') and col != 'CQ.Overall']\n",
    "        if cq_columns:\n",
    "            cq_stats = df[cq_columns].describe()\n",
    "            poor_quality_mask = (df[cq_columns] < 2).any(axis=1)\n",
    "            poor_quality_pct = (poor_quality_mask.sum() / len(df)) * 100\n",
    "            print(f\"Signal quality analysis:\")\n",
    "            print(f\"  - Poor quality samples: {poor_quality_mask.sum()} ({poor_quality_pct:.2f}%)\")\n",
    "\n",
    "            channel_quality = {}\n",
    "            for col in cq_columns:\n",
    "                ch_name = col.replace('CQ.', '')\n",
    "                poor_ch = (df[col] < 2).sum()\n",
    "                poor_ch_pct = (poor_ch / len(df)) * 100\n",
    "                channel_quality[ch_name] = (poor_ch_pct, df[col].mean())\n",
    "\n",
    "            print(\"  - Channels with poorest quality:\")\n",
    "            worst_channels = sorted(channel_quality.items(), key=lambda x: x[1][0], reverse=True)[:3]\n",
    "            for ch, (pct, mean_q) in worst_channels:\n",
    "                print(f\"    * {ch}: {pct:.2f}% poor quality, avg quality: {mean_q:.2f}\")\n",
    "\n",
    "        data = df[eeg_channels].values.T\n",
    "\n",
    "        ch_names = [ch.replace('EEG.', '') for ch in eeg_channels]\n",
    "\n",
    "        if 'OriginalTimestamp' in df.columns:\n",
    "            timestamps = df['OriginalTimestamp'].values\n",
    "            sfreq = 1.0 / np.median(np.diff(timestamps))\n",
    "        else:\n",
    "            sfreq = 128.0\n",
    "\n",
    "        print(f\"Sampling frequency: {sfreq:.2f} Hz\")\n",
    "\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "        raw = mne.io.RawArray(data, info)\n",
    "\n",
    "        try:\n",
    "            montage = mne.channels.make_standard_montage('standard_1020')\n",
    "            raw.set_montage(montage, match_case=False)\n",
    "            print(\"Successfully set 10-20 montage\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not set montage: {str(e)}\")\n",
    "\n",
    "        print(\"\\nApplying preprocessing steps:\")\n",
    "        raw_filtered = raw.copy().filter(l_freq=0.5)\n",
    "        print(\"  - Applied band-pass filter (1-40 Hz)\")\n",
    "\n",
    "        raw_notch = raw_filtered.copy().notch_filter(freqs=[50, 60])\n",
    "        print(\"  - Applied notch filter (50/60 Hz)\")\n",
    "\n",
    "        print(\"\\nSpectral analysis:\")\n",
    "        bands = {\n",
    "            'delta': (0.5, 4),\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 12),\n",
    "            'beta': (12, 30),\n",
    "            'gamma': (30, 640) # no max\n",
    "        }\n",
    "\n",
    "        band_powers = {}\n",
    "        for band_name, (fmin, fmax) in bands.items():\n",
    "            psd = raw_notch.compute_psd(method=\"welch\", fmin=fmin, fmax=fmax,\n",
    "                                      n_fft=int(sfreq * 2), n_overlap=int(sfreq))\n",
    "            print(\"psd:\", psd)\n",
    "            print(\"psd type:\", type(psd))\n",
    "            #Is psd a number or a time series? we want both\n",
    "\n",
    "            psds_data = psd.get_data()\n",
    "            freqs = psd.freqs\n",
    "\n",
    "            band_powers[band_name] = np.mean(psds_data, axis=1)\n",
    "\n",
    "        total_power = sum(np.mean(powers) for powers in band_powers.values())\n",
    "        for band, powers in band_powers.items():\n",
    "            rel_power = np.mean(powers) / total_power * 100\n",
    "            print(f\"  - {band.capitalize()} power: {rel_power:.2f}%\")\n",
    "\n",
    "        dominant_band = max(band_powers.items(), key=lambda x: np.mean(x[1]))\n",
    "        print(f\"  - Dominant frequency band: {dominant_band[0].capitalize()}\")\n",
    "\n",
    "        print(\"\\nRunning ICA for artifact removal:\")\n",
    "        ica = ICA(n_components=min(15, len(ch_names)), random_state=42)\n",
    "        ica.fit(raw_notch)\n",
    "\n",
    "        try:\n",
    "            eog_indices, eog_scores = ica.find_bads_eog(raw_notch)\n",
    "            if eog_indices:\n",
    "                ica.exclude = eog_indices\n",
    "                print(f\"  - Identified {len(eog_indices)} components related to eye artifacts\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Could not automatically identify eye artifacts: {str(e)}\")\n",
    "\n",
    "        raw_ica = raw_notch.copy()\n",
    "        ica.apply(raw_ica)\n",
    "        print(\"  - Successfully applied ICA correction\")\n",
    "\n",
    "        if identifier in logs and logs[identifier]:\n",
    "            print(f\"\\nEvent analysis for recording {identifier}:\")\n",
    "\n",
    "            first_timestamp = df['OriginalTimestamp'].iloc[0] if 'OriginalTimestamp' in df.columns else df.index[0]\n",
    "\n",
    "            event_types = {}\n",
    "            for timestamp, color, description in logs[identifier]:\n",
    "                event_type = None\n",
    "                if '+' in description:\n",
    "                    event_type = 'cross'\n",
    "                elif 'first word' in description.lower() and color == 'r':\n",
    "                    event_type = 'first_word_light'\n",
    "                elif 'first word' in description.lower() and color == 'b':\n",
    "                    event_type = 'first_word_dark'\n",
    "                elif 'second word' in description.lower() and color == 'r':\n",
    "                    event_type = 'second_word_light'\n",
    "                elif 'second word' in description.lower() and color == 'b':\n",
    "                    event_type = 'second_word_dark'\n",
    "\n",
    "                if event_type:\n",
    "                    event_types[event_type] = event_types.get(event_type, 0) + 1\n",
    "\n",
    "            for event_type, count in event_types.items():\n",
    "                print(f\"  - {event_type}: {count} occurrences\")\n",
    "\n",
    "            words = set()\n",
    "            for _, _, description in logs[identifier]:\n",
    "                if \"word '\" in description.lower():\n",
    "                    parts = description.split(\"'\")\n",
    "                    if len(parts) >= 2:\n",
    "                        word = parts[1].strip()\n",
    "                        words.add(word)\n",
    "\n",
    "            if words:\n",
    "                print(f\"  - Unique words presented: {', '.join(words)}\")\n",
    "        else:\n",
    "            print(f\"\\nNo event logs found for recording {identifier}\")\n",
    "\n",
    "        print(\"\\nGenerating visualization...\")\n",
    "\n",
    "        data_clean = raw_ica.get_data()\n",
    "        times = raw_ica.times\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        channels_to_plot = min(5, len(ch_names))\n",
    "        channel_indices = np.linspace(0, len(ch_names)-1, channels_to_plot).astype(int)\n",
    "\n",
    "        for i in channel_indices:\n",
    "            ch = ch_names[i]\n",
    "            # Scale and offset each channel for better visualization\n",
    "            scaled_data = data_clean[i] * 1e6  # Convert to µV\n",
    "            offset = i * 50  # Offset for visualization\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=times,\n",
    "                y=scaled_data + offset,\n",
    "                name=ch,\n",
    "                line=dict(width=1)\n",
    "            ))\n",
    "\n",
    "        if identifier in logs and logs[identifier]:\n",
    "          for timestamp, color, description in logs[identifier]:\n",
    "              event_time = timestamp - first_timestamp\n",
    "\n",
    "              if event_time >= 0 and event_time <= times[-1]:\n",
    "                  marker_text = \"\"\n",
    "                  marker_color = color\n",
    "\n",
    "                  if '+' in description:\n",
    "                      marker_text = \"+\"\n",
    "                  elif \"lightblue\" in description.lower():\n",
    "                      marker_text = \"overt\"\n",
    "                  elif \"blue\" in description.lower():\n",
    "                      marker_text = \"covert\"\n",
    "\n",
    "                  try:\n",
    "                      fig.add_vline(\n",
    "                          x=event_time,\n",
    "                          line=dict(color=marker_color, width=1, dash=\"dash\"),\n",
    "                          annotation_text=marker_text,\n",
    "                          annotation_position=\"top right\"\n",
    "                      )\n",
    "                  except ValueError as e:\n",
    "                      print(f\"  - Warning: Could not add marker at {event_time}s: {str(e)}\")\n",
    "                      fig.add_vline(\n",
    "                          x=event_time,\n",
    "                          line=dict(color=\"gray\", width=1, dash=\"dash\"),\n",
    "                          annotation_text=marker_text,\n",
    "                          annotation_position=\"top right\"\n",
    "                      )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"Clean EEG Data with Event Markers - {os.path.basename(file_path)}\",\n",
    "            xaxis_title=\"Time (s)\",\n",
    "            yaxis_title=\"Amplitude (µV + offset)\",\n",
    "            height=600,\n",
    "            showlegend=True\n",
    "        )\n",
    "\n",
    "        output_file = f\"{results_dir}/{os.path.basename(file_path).replace('.csv', '_processed.html')}\"\n",
    "        fig.write_html(output_file)\n",
    "        print(f\"Interactive visualization saved to: {output_file}\")\n",
    "\n",
    "        print(\"\\nSUMMARY FINDINGS:\")\n",
    "        print(f\"  - Recording duration: {times[-1]:.2f} seconds\")\n",
    "        print(f\"  - Channels analyzed: {len(ch_names)}\")\n",
    "\n",
    "        # Calculate SNR (signal-to-noise ratio) - simplified approach\n",
    "        # Using alpha band power during fixation vs. during word presentation as a proxy\n",
    "        if identifier in logs and logs[identifier]:\n",
    "            try:\n",
    "                cross_periods = []\n",
    "                word_periods = []\n",
    "\n",
    "                for timestamp, _, description in logs[identifier]:\n",
    "                    rel_time = timestamp - first_timestamp\n",
    "                    if rel_time >= 0 and rel_time <= times[-1]:\n",
    "                        time_idx = int(rel_time * sfreq)\n",
    "                        if '+' in description:\n",
    "                            end_idx = min(time_idx + int(sfreq), len(times))\n",
    "                            cross_periods.append((time_idx, end_idx))\n",
    "                        elif 'word' in description.lower() and not 'again' in description.lower():\n",
    "                            end_idx = min(time_idx + int(2 * sfreq), len(times))\n",
    "                            word_periods.append((time_idx, end_idx))\n",
    "\n",
    "                if cross_periods and word_periods:\n",
    "                    alpha_cross = []\n",
    "                    for start, end in cross_periods:\n",
    "                        for ch in range(len(ch_names)):\n",
    "                            # Simple FFT-based power in 8-13 Hz\n",
    "                            if end - start > 0:  # Make sure window has data\n",
    "                                segment = data_clean[ch, start:end]\n",
    "                                fft_vals = np.abs(np.fft.rfft(segment))\n",
    "                                fft_freq = np.fft.rfftfreq(len(segment), 1.0/sfreq)\n",
    "\n",
    "                                # Get alpha band power\n",
    "                                alpha_idx = np.logical_and(fft_freq >= 8, fft_freq <= 13)\n",
    "                                alpha_power = np.mean(fft_vals[alpha_idx]**2)\n",
    "                                alpha_cross.append(alpha_power)\n",
    "\n",
    "                    # Calculate alpha power during word presentation\n",
    "                    alpha_word = []\n",
    "                    for start, end in word_periods:\n",
    "                        for ch in range(len(ch_names)):\n",
    "                            if end - start > 0:  # Make sure window has data\n",
    "                                segment = data_clean[ch, start:end]\n",
    "                                fft_vals = np.abs(np.fft.rfft(segment))\n",
    "                                fft_freq = np.fft.rfftfreq(len(segment), 1.0/sfreq)\n",
    "\n",
    "                                # Get alpha band power\n",
    "                                alpha_idx = np.logical_and(fft_freq >= 8, fft_freq <= 13)\n",
    "                                alpha_power = np.mean(fft_vals[alpha_idx]**2)\n",
    "                                alpha_word.append(alpha_power)\n",
    "\n",
    "                    if alpha_cross and alpha_word:\n",
    "                        alpha_cross_mean = np.mean(alpha_cross)\n",
    "                        alpha_word_mean = np.mean(alpha_word)\n",
    "\n",
    "                        # Calculate alpha ERD (event-related desynchronization)\n",
    "                        alpha_erd_pct = ((alpha_cross_mean - alpha_word_mean) / alpha_cross_mean) * 100\n",
    "\n",
    "                        if alpha_erd_pct > 0:\n",
    "                            print(f\"  - Alpha ERD during word presentation: {alpha_erd_pct:.2f}%\")\n",
    "                            if alpha_erd_pct > 15:\n",
    "                                print(\"    * Strong alpha suppression suggests attentive processing\")\n",
    "                            elif alpha_erd_pct > 5:\n",
    "                                print(\"    * Moderate alpha suppression observed\")\n",
    "                            else:\n",
    "                                print(\"    * Minimal alpha suppression observed\")\n",
    "                        else:\n",
    "                            print(f\"  - Alpha ERS during word presentation: {-alpha_erd_pct:.2f}%\")\n",
    "                            print(\"    * Unusual alpha synchronization - potential artifact or unique task effect\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - Could not calculate alpha ERD/ERS: {str(e)}\")\n",
    "\n",
    "        return {\n",
    "            'success': True,\n",
    "            'file': os.path.basename(file_path),\n",
    "            'channels': len(ch_names),\n",
    "            'samples': len(times),\n",
    "            'duration': times[-1],\n",
    "            'events': len(logs.get(identifier, [])),\n",
    "            'visualization': output_file\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during processing: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'success': False,\n",
    "            'file': os.path.basename(file_path),\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning data directory...\n",
      "\n",
      "Found 3 valid files to process\n",
      "\n",
      "================================================================================\n",
      "Processing file: Aws Safi1742289648917_EPOCX_211983_2025.03.18T12.20.50+03.00.md.pm.bp.csv\n",
      "================================================================================\n",
      "Successfully read data: 86580 rows, 169 columns\n",
      "ERROR: No EEG channels found in data\n",
      "\n",
      "================================================================================\n",
      "Processing file: Aws Safi1742290353356_EPOCX_211983_2025.03.18T12.32.34+03.00.md.pm.bp.csv\n",
      "================================================================================\n",
      "Successfully read data: 86609 rows, 169 columns\n",
      "ERROR: No EEG channels found in data\n",
      "\n",
      "================================================================================\n",
      "Processing file: Aws Safi1742291163364_EPOCX_211983_2025.03.18T12.46.04+03.00.md.pm.bp.csv\n",
      "================================================================================\n",
      "Successfully read data: 86576 rows, 169 columns\n",
      "ERROR: No EEG channels found in data\n",
      "\n",
      "================================================================================\n",
      "FINAL PROCESSING SUMMARY\n",
      "================================================================================\n",
      "Total files processed: 0\n",
      "Successfully processed: 0\n",
      "Failed processing: 0\n",
      "\n",
      "All results saved to: results\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "name = 'Aws Safi'\n",
    "print(\"\\nScanning data directory...\")\n",
    "files_to_process = []\n",
    "\n",
    "for s in os.listdir(data_path):\n",
    "    if not s.endswith('.csv'):\n",
    "        continue\n",
    "    if not s.startswith(name):\n",
    "        continue\n",
    "\n",
    "    identifier = s.split(name)[1].split(\"_\")[0]\n",
    "    file_path = f\"{data_path}/{s}\"\n",
    "\n",
    "    is_valid, message = validate_csv(file_path)\n",
    "    if is_valid:\n",
    "        files_to_process.append((file_path, identifier))\n",
    "    else:\n",
    "        print(f\"Skipping invalid file {s}: {message}\")\n",
    "\n",
    "print(f\"\\nFound {len(files_to_process)} valid files to process\")\n",
    "\n",
    "results = []\n",
    "for file_path, identifier in files_to_process:\n",
    "    if identifier in logs:\n",
    "        result = preprocess_eeg(file_path, identifier, logs)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    else:\n",
    "        print(f\"Skipping {file_path} - no matching log file found\")\n",
    "\n",
    "successful = [r for r in results if r.get('success', False)]\n",
    "failed = [r for r in results if not r.get('success', False)]\n",
    "\n",
    "print(f\"\\n{'='*80}\\nFINAL PROCESSING SUMMARY\\n{'='*80}\")\n",
    "print(f\"Total files processed: {len(results)}\")\n",
    "print(f\"Successfully processed: {len(successful)}\")\n",
    "print(f\"Failed processing: {len(failed)}\")\n",
    "\n",
    "if successful:\n",
    "    print(\"\\nSuccessfully processed files:\")\n",
    "    for i, result in enumerate(successful, 1):\n",
    "        print(f\"{i}. {result['file']} - {result['channels']} channels, {result['duration']:.2f}s, {result['events']} events\")\n",
    "\n",
    "if failed:\n",
    "    print(\"\\nFailed files:\")\n",
    "    for i, result in enumerate(failed, 1):\n",
    "        print(f\"{i}. {result['file']} - Error: {result['error']}\")\n",
    "\n",
    "print(f\"\\nAll results saved to: {results_dir}\")\n",
    "print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
